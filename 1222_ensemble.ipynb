{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /home/work/.local/lib/python3.8/site-packages (4.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/work/.local/lib/python3.8/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/work/.local/lib/python3.8/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/work/.local/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2020.2.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.55.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/work/.local/lib/python3.8/site-packages (from huggingface-hub>=0.0.17->transformers) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Vm2GggmFqR3O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: adabelief-pytorch==0.2.0 in /home/work/.local/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.8/dist-packages (from adabelief-pytorch==0.2.0) (0.8.7)\n",
      "Requirement already satisfied: colorama>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from adabelief-pytorch==0.2.0) (0.4.3)\n",
      "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from adabelief-pytorch==0.2.0) (1.9.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in /home/work/.local/lib/python3.8/site-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (4.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install adabelief-pytorch==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (5.0.4)\n",
      "Collecting nbformat\n",
      "  Downloading nbformat-5.7.1-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fastjsonschema\n",
      "  Downloading fastjsonschema-2.16.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat) (3.2.0)\n",
      "Collecting traitlets>=5.1\n",
      "  Downloading traitlets-5.8.0-py3-none-any.whl (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 33.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat) (4.6.3)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat) (0.15.7)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat) (57.0.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat) (19.3.0)\n",
      "Installing collected packages: traitlets, fastjsonschema, nbformat\n",
      "\u001b[33m  WARNING: The script jupyter-trust is installed in '/home/work/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nbclient 0.5.1 requires jupyter-client>=6.1.5, but you have jupyter-client 6.0.0 which is incompatible.\u001b[0m\n",
      "Successfully installed fastjsonschema-2.16.2 nbformat-5.7.1 traitlets-5.8.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install nbformat # 설치 시도 \n",
    "!pip install -U nbformat # 기설치시, update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4748,
     "status": "ok",
     "timestamp": 1628547601355,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "NtqDJONzX9VU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import json\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from transformers import BertModel, TFBertModel, TFRobertaModel, RobertaTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForSequenceClassification\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold\n",
    "from adabelief_pytorch import AdaBelief\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import shutil\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpYsLbtkCx3B"
   },
   "source": [
    "\"klue/roberta-large\" 사용 시 \"klue/roberta-large\" 임베딩과 NUM_EPOCHS = 3\n",
    "\n",
    "\"monologg/kobigbird-bert-base\" 모델 사용시  \"monologg/kobigbird-bert-base\" 임베딩으로  NUM_EPOCHS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1628486750341,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "GKzmUkXxaNk3"
   },
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "L_RATE = 1e-5\n",
    "MAX_LEN = 128\n",
    "max_grad_norm=1\n",
    "log_interval=200\n",
    "NUM_CORES = os.cpu_count()\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "DATA_IN_PATH = './data'\n",
    "DATA_OUT_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3243,
     "status": "ok",
     "timestamp": 1628486727389,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "ngp2zofMaOCp"
   },
   "outputs": [],
   "source": [
    "#두가지 임베딩\n",
    "#\"klue/roberta-large\"\n",
    "#\"monologg/koelectra-base-v3-discriminator\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\", cache_dir='bert_ckpt', do_lower_case=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\", cache_dir='bert_ckpt', do_lower_case=False)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", cache_dir='bert_ckpt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1628547702106,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "KW7wflsOa86M"
   },
   "outputs": [],
   "source": [
    "PATH = 'numpy값'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2378,
     "status": "ok",
     "timestamp": 1628547732334,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "boaPQVIkbMuh"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2. Label Encoding (유형, 극성, 시제, 확실성)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "type_le = LabelEncoder()\n",
    "train[\"유형\"] = type_le.fit_transform(train[\"유형\"].values)\n",
    "\n",
    "\n",
    "polarity_le = LabelEncoder()\n",
    "train[\"극성\"] = polarity_le.fit_transform(train[\"극성\"].values)\n",
    "\n",
    "\n",
    "tense_le = LabelEncoder()\n",
    "train[\"시제\"] = tense_le.fit_transform(train[\"시제\"].values)\n",
    "\n",
    "\n",
    "certainty_le = LabelEncoder()\n",
    "train[\"확실성\"] = certainty_le.fit_transform(train[\"확실성\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1628464374844,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "ISVHHL-7rHav"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df_data = df\n",
    "    def __getitem__(self, index):\n",
    "        # get the sentence from the dataframe\n",
    "        sentence = self.df_data.loc[index, '문장']\n",
    "        encoded_dict = tokenizer(\n",
    "          text = sentence,\n",
    "          add_special_tokens = True, \n",
    "          max_length = MAX_LEN,\n",
    "          pad_to_max_length = True,\n",
    "          truncation=True,           # Pad & truncate all sentences.\n",
    "          return_tensors=\"pt\")\n",
    "\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        token_type_id = encoded_dict['token_type_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "        target = torch.tensor(self.df_data.loc[index, \"확실성\"])\n",
    "        sample = (padded_token_list, token_type_id , att_mask, target)\n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1628464376805,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "hsQDbdMKrIyn"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df_data = df\n",
    "    def __getitem__(self, index):\n",
    "        # get the sentence from the dataframe\n",
    "        sentence = self.df_data.loc[index, '문장']\n",
    "        encoded_dict = tokenizer(\n",
    "          text = sentence,\n",
    "          add_special_tokens = True, \n",
    "          max_length = MAX_LEN,\n",
    "          pad_to_max_length = True,\n",
    "          truncation=True,           # Pad & truncate all sentences.\n",
    "          return_tensors=\"pt\")\n",
    "\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        token_type_id = encoded_dict['token_type_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "        sample = (padded_token_list, token_type_id , att_mask)\n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1628464378445,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "TSFLp8fiq02N"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paWaosotByFp"
   },
   "source": [
    "h값으로 미세조정한 후 저장\n",
    "ex) Roberta229 = h값 229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7J2-zCLqbdCy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-17-37a64307fcf9>:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0986cc17617a4c0c9b8771527f0d1499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7336499691009521 train acc 0.193525641025641\n",
      "epoch 1 batch id 201 loss 0.36710068583488464 train acc 0.9052488664750682\n",
      "epoch 1 batch id 401 loss 0.06344197690486908 train acc 0.923247490981822\n",
      "epoch 1 train acc 0.9280391852041546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-37a64307fcf9>:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abe3fd4983b4991b4bc1f23b786140e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.3402310013771057 train acc 0.9152542372881356\n",
      "epoch 2 batch id 201 loss 0.21545369923114777 train acc 0.9478956747316013\n",
      "epoch 2 batch id 401 loss 0.29974889755249023 train acc 0.9461716294053685\n",
      "epoch 2 train acc 0.9461823113288008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-37a64307fcf9>:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498671ee37b9475fbc50f0f25a96ea9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.28200000524520874 train acc 0.9041666666666667\n",
      "epoch 3 batch id 201 loss 0.04326974228024483 train acc 0.9538954649048361\n",
      "epoch 3 batch id 401 loss 0.16976340115070343 train acc 0.9542177140868108\n",
      "epoch 3 train acc 0.9542198576228881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-37a64307fcf9>:94: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0e63fccc954b18ba6db54260fc690d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#332 : epoch 3 train acc 0.9187989758050054\n",
    "# 273 : epoch 3 train acc 0.9268683707635386\n",
    "# 325 : epoch 3 train acc 0.9205169095085536\n",
    "# 251 : epoch 3 train acc 0.9313042428174262\n",
    "# kobig120 : epoch 3 train acc 0.9093886905876433\n",
    "\n",
    "#극성\n",
    "#332 : epoch 3 train acc 0.9854487346754297\n",
    "#273 : epoch 3 train acc 0.9868469425354284\n",
    "#325 : epoch 3 train acc 0.984686812931019\n",
    "#251 : epoch 3 train acc 0.988702150120287\n",
    "# kobig120 : epoch 3 train acc 0.9812677962160321\n",
    "#시제\n",
    "#332 : epoch 3 train acc 0.919275517196581\n",
    "#273 : epoch 3 train acc 0.9312424963644657\n",
    "#325 : epoch 3 train acc 0.9180134893123439\n",
    "#251 : epoch 3 train acc 0.9349046320802006\n",
    "#\n",
    "\n",
    "#확실성\n",
    "# 332 : epoch 3 train acc 0.9496036063037623\n",
    "# 273 : epoch 3 train acc 0.9542198576228881\n",
    "# 325 : epoch 3 train acc 0.9492931109324948\n",
    "# 251 : epoch 3 train acc 0.9584442313345445\n",
    "#AUto \n",
    "train_data = TrainDataset(train)\n",
    "\n",
    "test_data = TestDataset(test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                      num_workers=NUM_CORES)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                      num_workers=NUM_CORES)\n",
    "#두가지 모델\n",
    "#\"klue/roberta-large\"\n",
    "#\"monologg/koelectra-base-v3-discriminator\"\n",
    "#kobigbird-bert-base\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"skt/kogpt2-base-v2\",um_labels=3)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-large\",num_labels=2)\n",
    "\n",
    "\n",
    "####미세조정\n",
    "n=0\n",
    "for name, child in model.named_children():\n",
    "    if n==0:\n",
    "      h=0\n",
    "      for param in child.parameters():\n",
    "        if h<=273: #이부분 숫자 조절로 fine-tuning => Roberta229: h=229\n",
    "          param.requires_grad = False\n",
    "        h+=1\n",
    "    n+=1\n",
    "#####\n",
    "    # print(param)\n",
    "model.to(device)\n",
    "optimizer = AdaBelief(model.parameters(), lr=1e-5, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False)\n",
    "\n",
    "warmup_ratio = 0.1\n",
    "t_total = len(train_dataloader) * NUM_EPOCHS\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    best_acc =0.0\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        input_id = input_id.long().to(device)\n",
    "        token_type_id = token_type_id.long().to(device)\n",
    "        attention_mask = attention_mask.long().to(device)\n",
    "        label = label.to(device)\n",
    "        outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask, labels=label)\n",
    "        loss = outputs[0]\n",
    "        out = outputs[1]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        _, preds = torch.max(out, dim=1)\n",
    "        train_acc += f1_score(preds.cpu(), label.cpu(),average='weighted')\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "\n",
    "preds = [] \n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    input_id = input_id.long().to(device)\n",
    "    token_type_id = token_type_id.long().to(device)\n",
    "    attention_mask = attention_mask.long().to(device)\n",
    "    outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask)\n",
    "    out = outputs[0]\n",
    "    for inp in out:\n",
    "      preds.append(inp.detach().cpu().numpy())\n",
    "Preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.argmax(Preds, axis=1)\n",
    "#submission['topic_idx']= results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_pred=type_le.inverse_transform(results)\n",
    "polarity_pred=polarity_le.inverse_transform(results)\n",
    "#tense_pred=tense_le.inverse_transform(results)\n",
    "#certainty_le_pred=certainty_le.inverse_transform(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['유형']= test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['긍정', '긍정', '긍정', ..., '긍정', '긍정', '긍정'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "사실형    7090\n",
       "Name: 유형, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.유형.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['극성']= polarity_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "긍정    6754\n",
       "부정     287\n",
       "미정      49\n",
       "Name: 극성, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.극성.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['시제']= tense_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "과거    3462\n",
       "현재    2961\n",
       "미래     667\n",
       "Name: 시제, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.시제.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['확실성']= certainty_le_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "확실     6627\n",
       "불확실     463\n",
       "Name: 확실성, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.확실성.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label']=submission.유형+'-'+submission.극성+'-'+submission.시제+'-'+submission.확실성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "last=submission[['ID','label']]\n",
    "last=last.set_index('ID')\n",
    "last.to_csv('klue_최종label.csv',encoding='utf-8-sig',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Roberta332_certainty=Preds \n",
    "np.save(PATH+'Roberta332_certainty.npy',arr=Roberta332_certainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1628481131997,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "ZPFBY530_vl4"
   },
   "outputs": [],
   "source": [
    "Roberta273_certainty=Preds \n",
    "np.save(PATH+'Roberta273_certainty.npy',arr=Roberta273_certainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1628472508243,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "Gw2sQcswPffH"
   },
   "outputs": [],
   "source": [
    "Roberta325_tense=Preds \n",
    "np.save(PATH+'Roberta325_tense.npy',arr=Roberta325_tense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Roberta251_certainty=Preds\n",
    "np.save(PATH+'Roberta251_certainty.npy',arr=Roberta251_certainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1628472510234,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "q45Om9fxHF33"
   },
   "outputs": [],
   "source": [
    "kobig120=Preds \n",
    "np.save(PATH+'Electra120.npy',arr=kobig120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#332 : epoch 3 train acc 0.9187989758050054\n",
    "# 273 : epoch 3 train acc 0.9268683707635386\n",
    "# 325 : epoch 3 train acc 0.9205169095085536\n",
    "# 251 : epoch 3 train acc 0.9313042428174262\n",
    "# kobig120 : epoch 3 train acc 0.9093886905876433\n",
    "\n",
    "#극성\n",
    "#332 : epoch 3 train acc 0.9854487346754297\n",
    "#273 : epoch 3 train acc 0.9868469425354284\n",
    "#325 : epoch 3 train acc 0.984686812931019\n",
    "#251 : epoch 3 train acc 0.988702150120287\n",
    "# kobig120 : epoch 3 train acc 0.9812677962160321\n",
    "#시제\n",
    "#332 : epoch 3 train acc 0.919275517196581\n",
    "#273 : epoch 3 train acc 0.9312424963644657\n",
    "#325 : epoch 3 train acc 0.9180134893123439\n",
    "#251 : epoch 3 train acc 0.9349046320802006\n",
    "#\n",
    "\n",
    "#확실성\n",
    "# 332 : epoch 3 train acc 0.9496036063037623\n",
    "# 273 : epoch 3 train acc 0.9539584020942307\n",
    "# 325 : epoch 3 train acc 0.9492931109324948\n",
    "# 251 : epoch 3 train acc 0.9584442313345445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobig120=np.load('numpy값Electra120.npy')\n",
    "Roberta251=np.load('numpy값Roberta251.npy')\n",
    "Roberta273=np.load('numpy값Roberta273.npy')\n",
    "Roberta325=np.load('numpy값Roberta325.npy')\n",
    "Roberta332=np.load('numpy값Roberta332_type.npy')\n",
    "\n",
    "Roberta332_polarity= np.load('numpy값Roberta332_polarity.npy')\n",
    "Roberta325_polarity= np.load('numpy값Roberta325_polarity.npy')\n",
    "Roberta273_polarity= np.load('numpy값Roberta273_polarity.npy')\n",
    "Roberta251_polarity= np.load('numpy값Roberta251_polarirty.npy')\n",
    "kobig120_polarity=np.load('numpy값kobig120_polarity.npy')\n",
    "\n",
    "Roberta332_tense= np.load('numpy값Roberta332_tense.npy')\n",
    "Roberta325_tense= np.load('numpy값Roberta325_tense.npy')\n",
    "Roberta273_tense= np.load('numpy값Roberta273_tense.npy')\n",
    "Roberta251_tense= np.load('numpy값Roberta251_tense.npy')\n",
    "kobig120_tense=np.load('numpy값kobig120_tense.npy')\n",
    "\n",
    "Roberta332_certainty= np.load('numpy값Roberta332_certainty.npy')\n",
    "Roberta325_certainty= np.load('numpy값Roberta325_certainty.npy')\n",
    "Roberta273_certainty= np.load('numpy값Roberta273_certainty.npy')\n",
    "Roberta251_certainty= np.load('numpy값Roberta251_certainty.npy')\n",
    "kobig120_certainty=np.load('numpy값kobig120_certainty.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1628547713256,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "5ICkbL9SG9qE"
   },
   "outputs": [],
   "source": [
    "Pred_values_type = Roberta332*0.1 + Roberta325 * 0.15 + Roberta273 * 0.15 + Roberta251 * 0.4 + kobig120 * 0.2\n",
    "Pred_values_polarity = Roberta332_polarity*0.1 + Roberta325_polarity * 0.15 + Roberta273_polarity * 0.15 + Roberta251_polarity * 0.4 + kobig120_polarity * 0.2\n",
    "Pred_values_tense = Roberta332_tense*0.1 + Roberta325_tense * 0.15 + Roberta273_tense * 0.15 + Roberta251_tense * 0.4 + kobig120_tense * 0.2\n",
    "Pred_values_certainty = Roberta332_certainty*0.1 + Roberta325_certainty * 0.15 + Roberta273_certainty * 0.15 + Roberta251_certainty * 0.4 + kobig120_certainty * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1628547737352,
     "user": {
      "displayName": "Light_Follower",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjHXMFY2SSYNzoS3D9k1xxJpnGnILpj0bc1SvPj=s64",
      "userId": "12253191562255143697"
     },
     "user_tz": -540
    },
    "id": "kwHLSFtYxOT_"
   },
   "outputs": [],
   "source": [
    "results_type = np.argmax(Pred_values_type, axis=1)\n",
    "type_pred_ang=type_le.inverse_transform(results_type)\n",
    "submission['유형']= type_pred_ang\n",
    "\n",
    "results_polarity = np.argmax(Pred_values_polarity, axis=1)\n",
    "polarity_pred_ang=polarity_le.inverse_transform(results_polarity)\n",
    "submission['극성']= polarity_pred_ang\n",
    "\n",
    "results_tense = np.argmax(Pred_values_tense, axis=1)\n",
    "tense_pred_ang=tense_le.inverse_transform(results_tense)\n",
    "submission['시제']= tense_pred_ang\n",
    "\n",
    "results_certainty = np.argmax(Pred_values_certainty, axis=1)\n",
    "certainty_pred_ang=certainty_le.inverse_transform(results_certainty)\n",
    "submission['확실성']= certainty_pred_ang\n",
    "\n",
    "submission['label']=submission.유형+'-'+submission.극성+'-'+submission.시제+'-'+submission.확실성\n",
    "last=submission[['ID','label']]\n",
    "last=last.set_index('ID')\n",
    "last.to_csv('klue_kobig_ang_최종label.csv',encoding='utf-8-sig',)\n",
    "#submission.to_csv(PATH + 'daconnews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>유형</th>\n",
       "      <th>극성</th>\n",
       "      <th>시제</th>\n",
       "      <th>확실성</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7085</th>\n",
       "      <td>TEST_7085</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7086</th>\n",
       "      <td>TEST_7086</td>\n",
       "      <td>추론형-긍정-현재-확실</td>\n",
       "      <td>추론형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7087</th>\n",
       "      <td>TEST_7087</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088</th>\n",
       "      <td>TEST_7088</td>\n",
       "      <td>추론형-긍정-미래-확실</td>\n",
       "      <td>추론형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>미래</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>TEST_7089</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7090 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID         label   유형  극성  시제 확실성\n",
       "0     TEST_0000  사실형-긍정-현재-확실  사실형  긍정  현재  확실\n",
       "1     TEST_0001  사실형-긍정-현재-확실  사실형  긍정  현재  확실\n",
       "2     TEST_0002  사실형-긍정-과거-확실  사실형  긍정  과거  확실\n",
       "3     TEST_0003  사실형-긍정-현재-확실  사실형  긍정  현재  확실\n",
       "4     TEST_0004  사실형-긍정-과거-확실  사실형  긍정  과거  확실\n",
       "...         ...           ...  ...  ..  ..  ..\n",
       "7085  TEST_7085  사실형-긍정-현재-확실  사실형  긍정  현재  확실\n",
       "7086  TEST_7086  추론형-긍정-현재-확실  추론형  긍정  현재  확실\n",
       "7087  TEST_7087  사실형-긍정-현재-확실  사실형  긍정  현재  확실\n",
       "7088  TEST_7088  추론형-긍정-미래-확실  추론형  긍정  미래  확실\n",
       "7089  TEST_7089  사실형-긍정-과거-확실  사실형  긍정  과거  확실\n",
       "\n",
       "[7090 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPeJ/igdBz6fD0yWoBPV9OH",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Dacon_news.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch 1.9.0 on Python 3.8 (CUDA 11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
